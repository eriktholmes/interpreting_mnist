{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "952a8d1c-823a-4463-84b7-d86b0c96a347",
   "metadata": {},
   "source": [
    "# 'Seeing' digits: an interpretability dive into MLPs and CNNs trained on MNIST data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdb99c9-a91c-4c27-a1e8-7c0b06db5f5b",
   "metadata": {},
   "source": [
    "## 1. The beginning\n",
    "I wanted to start a project/notebook focused specifically on interpretability—both to get a taste of the tools used in this area and to better understand the thought process behind analyzing what neural networks actually learn. Since my micrograd build ended with a failed MNIST experiment, this felt like the perfect place to pick things back up.\n",
    "\n",
    "> \"Failed\" might be the wrong word. Micrograd technically ran on MNIST, but it was painfully slow, and the resulting computation graph was almost comical. That said, building something from scratch and seeing it run—even slowly—was a great experiment. It gave me a better undestanding of the operations (like autodiff and backprop) that I will be using regualarly, and to see how actual ML frameworks like PyTorch optimize these operations.\n",
    "\n",
    "In this project, I’ll use basic PyTorch structures to mirror the MLP I built in micrograd, but with a modern framework and better tools. Along the way, I’ll add:\n",
    "\n",
    "- Capturing internal activations (via hooks)\n",
    "- Trying out dimensionality reduction methods like PCA\n",
    "- Visualizing how neurons respond to different inputs\n",
    "\n",
    "I don’t claim to fully understand all of these tools yet, but part of the point is to learn by doing, document that process clearly, and build a foundation I can use in future projects. Once the MLP is running I’ll upgrade to a CNN to see what kinds of differences emerge, both in performance and (hopefully) in how the network learns to represent digits.\n",
    "\n",
    "The broader goal is to build a strong foundation in the interpretability toolkit so I can apply these ideas to current (and future!) projects—like my toy AlphaZero agent, my LLM alignment project (to come!), and maybe even visual experiments with TransformerLens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256d12c6-d5d9-4b14-b444-55cfe8ce563d",
   "metadata": {},
   "source": [
    "## 2. The Multi-layer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf5f7f-f5e0-4a6f-aa83-bee1e6c52601",
   "metadata": {},
   "source": [
    "### Overview:\n",
    "I'm currently writing a more in-depth educational post on MLPs, building them from the ground up with full mathematical motivation. For this notebook, I’ll keep things fairly lightweight and just outline the key structures and operations so we can understand what’s happening with minimal background.\n",
    "\n",
    "An *MLP* is one of the simplest machine learning models—made up of a few key building blocks: basic linear algebra (vector sums and matrix multiplication), a nonlinear activation function, and a mechanism for learning via backpropagation. By stacking layers of *neurons* (perceptrons) where each output depends on all inputs from the previous layer, we build what's called a **fully connected neural network**. Each operation we use is differentiable, which means we can not only pass data forward through the network but also compute how to adjust the model using gradients and optimization as we go through training data.\n",
    "\n",
    "\n",
    "\n",
    "<!--I am currently writing a much more indepth explaination of MLPs (from the ground up with mathematical motivations) for educational/learning purposes so I will leave most of that detail for a blog post. Here we will just outline the important structures so we know what is going on with little background required. \n",
    "\n",
    "An *MLP* is a basic machine learning model that requires only a few building blocks and little math knowledge (or none... technically). Really, it is just basic linear algebra (vector sums/matrix multiplication) and some non-linear activation functions that make up a perceptron (aka a neuron). We then arrange a bunch of neurons into a layer and stack a bunch of layers together to give us a fully connected system where each operation is differentiable--in the sense of calculus--so we can not only pass information forward but also pass information backwards to adjust the model. Let's dig into the specifics breifly starting with a neuron:-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ecfceb-ad13-4b45-a4e3-939927cbe456",
   "metadata": {},
   "source": [
    "### 2.1. The perceptron (aka the neuron): "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be367802-b653-42f4-aa76-f1f4179b8cd7",
   "metadata": {},
   "source": [
    "We should just think of a perceptron (neuron) as a function that takes in many inputs and outputs a single number. Inside a neuron are a list of numbers (initially randomized and then updated as the model trains) called *weights*--one for each input to the neuron--and an optional bias (single number). The neuron takes in a list of numbers called the input in the form of a vector, multiplies each input with a weight and adds up all the results, then adds a bias to the result. This gives us a single number $y$, to which we can then apply a non-linear function $f$ to to get the output of the neuron $\\hat{y}$.\n",
    "> **Example**. Our input will be three dimensional: $x_1=4, x_2=8, x_3=7$. Our weights will be variables $w_1, w_2, w_3$ and bias $b$. The first step is to multiply all the inputs and weights together, take the sum and add a bias: $$x_1\\cdot w_1 + x_2\\cdot w_2 + x_3\\cdot w_3 + b= \\boxed{4w_1 + 8w_2 + 7w_3 + b = y}.$$ Note that the output $y$ is a single number. Lastly, we take some non-linear function $f$ and apply it to $y$. For this example we take the hyperbolic tangent $$f(x) = \\tanh(x) = \\frac{e^{2x}-1}{e^{2x}+1}$$ (where $e^x$ is the exponential function). This function takes values between $-1$ and $1$, and so we think of the values close to $1$ as indicating a strong activation and values close to $-1$ as very little activation (for the neuron analogy anyways!). The end result, i.e. the output of this neuron will be\n",
    "> $$ \\hat{y} = \\frac{e^{2y} - 1}{e^{2y} + 1} = f\\left(\\sum_{i=1}^3 (w_i\\cdot x_i) + b\\right)$$.\n",
    "\n",
    "So, the neuron takes in many inputs (say $n$) and outputs a single number $\\hat{y}$ which is generally the result of a specific function which introduces non-linearity. The reason for the *activation* function is that we wouldn't be able to model any non-linear data if we didn't have this function... everything would be the result of linear algebra (hence linear!)--even in a complicated model with many layers and MANY neurons--and so we really do need this for practical purposes. There are tons of options for this non-linear function and each has benefits/drawbacks: see [wikipedia](https://en.wikipedia.org/wiki/Activation_function) for more on activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "ecde9f4a-c209-4fa9-99cb-65c37fd3e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "# OPTION 1: direct(ish) port from micrograd\n",
    "# the neuron class takes in a number of inputs, nin, initializes the (randn = normally distributed) random weights and bias\n",
    "# calling the Neuron requires a list of nin inputs, x, and outputs tanh(y) if nonlin=True, otherwise outputs the linear result y = x*w + b\n",
    "\n",
    "class Neuron1:\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = torch.randn(nin, requires_grad=True)\n",
    "        self.b = torch.randn(1,   requires_grad=True)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = x @ self.w + self.b\n",
    "        return torch.tanh(y) if self.nonlin else y\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.w, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fb83b908-9b86-43bf-b68a-3c883acbae3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron1 parameters: [tensor([-0.8774,  1.8562, -1.3348], requires_grad=True), tensor([1.2367], requires_grad=True)]\n",
      "For the input tensor([4., 8., 7.]) we obtain an output of: 0.9968927502632141\n"
     ]
    }
   ],
   "source": [
    "N1 = Neuron1(3)\n",
    "\n",
    "print(f'Neuron1 parameters: {N1.parameters()}')\n",
    "\n",
    "x = torch.tensor([4,8,7]).float()\n",
    "\n",
    "print(f'For the input {x} we obtain an output of: {N1(x).item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c39598d6-fe17-49f7-ad5c-7ba1a6721f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Option 2: utilize the nn.Module class and its efficiencies\n",
    "# These look almost identical as written except we don't need to manually create weights and bias, nor do we directly compute the linear output\n",
    "# We also don't need to define the neurons parameters as this is all done inside of the nn.Module! \n",
    "\n",
    "class Neuron2(nn.Module):\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(nin, 1)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        return torch.tanh(y) if self.nonlin else y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e89a5f67-f0d2-4d64-b4d7-1844cd315bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron2 parameters:\n",
      "  linear.weight:[-0.3273866  -0.45594507  0.06844634]\n",
      "  linear.bias:[-0.5026901]\n",
      "For the input tensor([4., 8., 7.]) we obtain an output of: -0.9999056458473206\n"
     ]
    }
   ],
   "source": [
    "N2 = Neuron2(3)\n",
    "\n",
    "print(\"Neuron2 parameters:\")\n",
    "for name, param in N2.named_parameters():\n",
    "    print(f\"  {name}:{param.data.numpy().flatten()}\")\n",
    "\n",
    "x = torch.tensor([4,8,7]).float()\n",
    "\n",
    "print(f'For the input {x} we obtain an output of: {N2(x).item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade56e9c-7a9b-4db7-9db8-cba82991f0b4",
   "metadata": {},
   "source": [
    "### 2.2. The layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97e09b-40e6-4cce-9f41-2bcadc16f712",
   "metadata": {},
   "source": [
    "**Key idea:** A *layer* is a collection of neurons that all receive the same input vector and independently compute their outputs.\n",
    "\n",
    "Each neuron has its own set of weights and bias, and produces a single output like we outlined above. So, if a layer has $m$ neurons and receives an $n$-dimensional input, the output will be an $m$-dimensional vector. Here is a little sketch of the flow through a layer:\n",
    "\n",
    "$$ \\begin{align*}\n",
    "    (x_1, x_2, \\dots, x_n)  \\rightarrow &\\fbox{Layer}  \\rightarrow  (\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_m)\n",
    "    \\end{align*}\n",
    "$$\n",
    "> **Example (continued):** Above we had a simple example of a neuron with $\\tanh$ activation. To form a layer we can just link a few neurons together to produce an output. Let's use 2 neurons to keep things simple: each neuron has 3 weights and a bias and these will generally not be the same. We can add subscripts to distinguish between the weights and bias of each neuron:\n",
    "    $$\n",
    "        \\begin{align*}\n",
    "                n_1 : (w_{1,1},\\; w_{1,2},\\; w_{1,3},\\;\\; b_1) \\\\\n",
    "                n_2 : (w_{2,1},\\; w_{2,2},\\; w_{2,3},\\;\\; b_2)\n",
    "        \\end {align*}\n",
    "    $$\n",
    "> so the first number represents that neuron and the second represents the weight in that neuron. Assuming the same inputs as the previous example, the result of this layer will be\n",
    "    $$\n",
    "        \\begin{align*}\n",
    "                \\hat{y}_1 : \\tanh(4w_{1,1} + 8w_{1,2} + 7w_{1,3} + b_1) = f\\left( \\sum_{i=1}^3 (w_{1,i}\\cdot x_i ) + b_1\\right) \\\\\n",
    "                \\hat{y}_2 : \\tanh(4w_{2,1} + 8w_{2,2} + 7w_{2,3} + b_2) = f\\left( \\sum_{i=1}^3 (w_{2,i}\\cdot x_i ) + b_2\\right)\n",
    "        \\end {align*}\n",
    "    $$\n",
    "> or, using matrices/vectors we can write the weights, bias and input as:\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    W_1 = \\begin{pmatrix} w_{1,1} & w_{1,2} & w_{1,3} \\\\ w_{2,1} & w_{2,2} & w_{2,3} \\end{pmatrix}, \n",
    "    && B_1 = \\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix}, \n",
    "    && X = \\begin{pmatrix} 4 \\\\ 8 \\\\ 7 \\end{pmatrix}.\n",
    "    \\end{align*}\n",
    "    $$\n",
    "> And using this, the operations of layer 1 can be written succinctly using linear algebra:\n",
    "    $$ f\\left( W_1\\cdot X + B_1\\right). $$\n",
    "\n",
    "The notation that we used at the end of the example generalizes nicely to arbitrary dimensions. If there are $n$ inputs to the layer, and $m$ neurons in the layer then $W_1$ is a matrix (or array) with $m$-rows and $n$ columns, $X$ is a column vector (a 1-dimensional array) with $n$-rows, and $B_1$ is a column vector with $m$-rows. The activation function then applies to each row in the result $Y$ which is a column vector with $m$-rows (one for each neurons output):\n",
    "    $$ f(W_1\\cdot X + B_1) = f(Y) = \\hat(Y). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c776340d-499f-466e-afb1-ac67e6c8dcac",
   "metadata": {},
   "source": [
    "### 2.3. The Multi-Layer Perceptron:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf61527-5d88-445e-b33d-a3ff0c9640dc",
   "metadata": {},
   "source": [
    "An MLP is just a series of *fully connected* layers. The only requirement is that the output size of one layer matches the input size of the next—otherwise, the data can't flow forward.\n",
    "\n",
    "Here’s a diagram of a simple MLP with 3 inputs, two hidden layers (with 4 and 6 neurons respectively), and a single output neuron:\n",
    "> <img src=\"basic_mlp.png\" style=\"width: 500px; height: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6580ba-dca8-4101-8940-a612073bf3f9",
   "metadata": {},
   "source": [
    "### 2.4: Training an MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e59f82-887d-403c-82be-179b6d828e86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.4.1: On the level of neurons\n",
    "1) **Feedforward**: The neuron computes a weighted sum of its inputs plus a bias term:\n",
    "       $$y = x\\cdot w + b$$\n",
    "   It then optionally applies a nonlinear activation function (e.g. tanh, ReLU) to produce the output:\n",
    "       $$\\hat{y} = activation(y)$$\n",
    "\n",
    "2) **Loss calculation**: The output from the forward pass is compared to the target label using a loss function, which outputs a scalar that quantifies how far off the prediction is.\n",
    "> For example, for a regression task we might use mean squared error:\n",
    "    $$ \\mathcal{L} = (y_{pred} - y_{true})^2$$\n",
    "\n",
    "3) **Compute gradients (via backpropagation)**:\n",
    "Using the chain rule from calculus, we can compute the gradient of the loss with respect to each parameter (i.e. how much changing that parameter would affect the loss). These gradients tell us how to adjust each weight and bias to make the loss smaller.\n",
    "This process — applying the chain rule backward through the entire computation graph — is called **backpropagation**. In PyTorch, it's triggered by calling ```loss.backward()```. This automatically populates the ```.grad``` field of each parameter with its respective gradient.\n",
    "\n",
    "> **Note**: The gradient points in the direction of steepest increase in the loss function. Since we want to minimize the loss, we take a step in the *opposite* direction-this is **gradient descent**.\n",
    "   \n",
    "4) **Update the parameters**:\n",
    "After computing gradients, we update the weights and biases using **gradient descent**: each parameter is adjusted by subtracting a small fraction (i.e. the learning rate, $\\alpha$) of its gradient:\n",
    "> Mathematically we are doing the following:\n",
    "    $$ w_i\\rightarrow w_i - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w_i} $$\n",
    "    $$ b\\rightarrow b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$$\n",
    "\n",
    "    In PyTorch, this can be done manually:\n",
    "\n",
    "    > ```python\n",
    "    with torch.no_grad():\n",
    "        for p in model.parameters():\n",
    "            p -= alpha * p.grad\n",
    "    ```\n",
    "\n",
    "5) **Repeat**:  \n",
    "This process is repeated for many iterations (epochs) across the dataset, gradually refining the parameters to minimize the loss and improve model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13aad5-a265-4be3-a38a-452bde180b6c",
   "metadata": {},
   "source": [
    "### 2.5: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a840d29-c8b7-4889-8358-4557fe8b132b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2.5.1. Neurons in pytorch (two ways!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2200f3e2-3f04-4afd-b5dd-bf907e370ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67bc294b-9445-43c0-bb1d-a0aa14e05448",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. MLP Implementation on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24081ac2-6558-462b-b402-5dd76ffefdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea3270f5-526e-43bd-a68a-6b2eba089235",
   "metadata": {},
   "source": [
    "## 4. Interpretability Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6934f-dcfd-47fc-a4a6-57e787e15f96",
   "metadata": {},
   "source": [
    "## 5. The Convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4823781-28b7-4e62-a8dc-2a4237882dfd",
   "metadata": {},
   "source": [
    "## 6. MNIST upgraded via CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43534a6-d112-44db-a6f4-cc5ff9c648e9",
   "metadata": {},
   "source": [
    "## 7. Reflections + future directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea67eb0-cc92-458b-97d8-c53adb03292e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
